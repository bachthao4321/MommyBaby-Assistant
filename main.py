import os
import json
import streamlit as st
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from vectorize_documents import embeddings

# Thi·∫øt l·∫≠p th∆∞ m·ª•c l√†m vi·ªác v√† ƒë·ªçc d·ªØ li·ªáu c·∫•u h√¨nh
working_dir = os.path.dirname(os.path.abspath(__file__))
config_data = json.load(open(f"{working_dir}/config.json"))
GROQ_API_KEY = config_data["GROQ_API_KEY"]
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

# T·∫°o m·ªôt t·ª´ ƒëi·ªÉn ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ c√°c phi√™n tr√≤ chuy·ªán
session_store = {}

def get_session_history(session_id):
    """Tr·∫£ v·ªÅ l·ªãch s·ª≠ tr√≤ chuy·ªán cho m·ªôt phi√™n nh·∫•t ƒë·ªãnh."""
    if session_id not in session_store:
        session_store[session_id] = ChatMessageHistory()  # Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng ChatMessageHistory
    return session_store[session_id]

def setup_vectorstore():
    persist_directory = f"{working_dir}/vector_db_dir"
    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    return vectorstore

def chat_chain(vectorstore):
    # Thi·∫øt l·∫≠p template cho prompt
    qa_system_prompt = """You are MommyBaby, a virtual assistant that provides nutritional advice about milk for mothers and babies. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Use three sentences maximum and keep the answer concise.\

    {context}"""
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0)
    
    # T·∫°o retrieval chain
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    documents_chain = create_stuff_documents_chain(llm, qa_prompt)
    retrieval_chain = create_retrieval_chain(retriever, documents_chain)
    
    # T·∫°o runnable v·ªõi l·ªãch s·ª≠ th√¥ng ƒëi·ªáp
    conservation_chain = RunnableWithMessageHistory(
        retrieval_chain,
        get_session_history,  
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    )
    
    return conservation_chain

# C·∫•u h√¨nh giao di·ªán Streamlit
st.set_page_config(
    page_title="MommyBaby",
    page_icon="üìö",
    layout="centered"
)

st.title("üìö MommyBaby Chatbot")

# Kh·ªüi t·∫°o tr·∫°ng th√°i chat_history n·∫øu ch∆∞a c√≥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Kh·ªüi t·∫°o vectorstore n·∫øu ch∆∞a c√≥
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = setup_vectorstore()

# Kh·ªüi t·∫°o conversational_chain n·∫øu ch∆∞a c√≥
if "retrieval_chain" not in st.session_state:
    st.session_state.retrieval_chain = chat_chain(st.session_state.vectorstore)

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Nh·∫≠p c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
user_input = st.chat_input("Ask AI...")

if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    # G·ªçi retrieval chain v·ªõi l·ªãch s·ª≠ tr√≤ chuy·ªán
    with st.chat_message("assistant"):
        response = st.session_state.retrieval_chain.invoke(
            {"input": user_input}, 
            config={"configurable": {"session_id": "default"}}  # S·ª≠ d·ª•ng session_id cho m·ªói phi√™n
        )
        
        # Ki·ªÉm tra xem ph·∫£n h·ªìi c√≥ ch·ª©a c√¢u tr·∫£ l·ªùi kh√¥ng
        if 'answer' in response:
            assistant_response = response["answer"]
            st.markdown(assistant_response)
            st.session_state.chat_history.append({"role": "assistant", "content": assistant_response})

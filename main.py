import os
import json

import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Thi·∫øt l·∫≠p th∆∞ m·ª•c l√†m vi·ªác v√† ƒë·ªçc d·ªØ li·ªáu c·∫•u h√¨nh
working_dir = os.path.dirname(os.path.abspath(__file__))
config_data = json.load(open(f"{working_dir}/config.json"))
GROQ_API_KEY = config_data["GROQ_API_KEY"]
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

def setup_vectorstore():
    persist_directory = f"{working_dir}/vector_db_dir"
    embeddings = HuggingFaceEmbeddings()
    vectorstore = Chroma(persist_directory=persist_directory,
                         embedding_function=embeddings)
    return vectorstore

def chat_chain(vectorstore):
    # Thi·∫øt l·∫≠p template cho prompt
    prompt_template = PromptTemplate(
        input_variables=["chat_history", "question"],
        template="MommyBaby l√† tr·ª£ l√Ω ·∫£o c·ªßa b·∫°n trong vi·ªác cung c·∫•p th√¥ng tin v·ªÅ c√°c s·∫£n ph·∫©m s·ªØa ch·∫•t l∆∞·ª£ng.\n"
                 "Cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√¢y:\n{chat_history}\n"
                 "Ng∆∞·ªùi d√πng: {question}\n"
                 "MommyBaby Assistant:"
    )

    llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0.3)
    retriever = vectorstore.as_retriever()
    memory = ConversationBufferMemory(
        llm=llm,
        output_key="answer",
        memory_key="chat_history",
        return_messages=True
    )

    # T·∫°o m·ªôt LLMChain v·ªõi prompt template
    chain = LLMChain(
        llm=llm,
        prompt=prompt_template
    )

    return chain  # Ch·ªâ tr·∫£ v·ªÅ chain

# C·∫•u h√¨nh giao di·ªán Streamlit
st.set_page_config(
    page_title="Multi Doc Chat",
    page_icon="üìö",
    layout="centered"
)

st.title("üìö Multi Documents Chatbot")

# Kh·ªüi t·∫°o tr·∫°ng th√°i chat_history n·∫øu ch∆∞a c√≥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Kh·ªüi t·∫°o vectorstore n·∫øu ch∆∞a c√≥
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = setup_vectorstore()

# Kh·ªüi t·∫°o conversational_chain n·∫øu ch∆∞a c√≥
if "conversational_chain" not in st.session_state:
    st.session_state.conversational_chain = chat_chain(st.session_state.vectorstore)

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Nh·∫≠p c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
user_input = st.chat_input("Ask AI...")

if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    # ƒê·ªãnh d·∫°ng l·∫°i l·ªãch s·ª≠ tr√≤ chuy·ªán
    formatted_history = "\n".join(
        [f"{msg['role'].capitalize()}: {msg['content']}" for msg in st.session_state.chat_history]
    )

    with st.chat_message("assistant"):
        # G·ªçi chain v·ªõi l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë√£ ƒë·ªãnh d·∫°ng
        response = st.session_state.conversational_chain({"question": user_input, "chat_history": formatted_history})
        assistant_response = response["text"]
        st.markdown(assistant_response)
        st.session_state.chat_history.append({"role": "assistant", "content": assistant_response})
